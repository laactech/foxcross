{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Foxcross AsyncIO serving for data science models built on Starlette Requirements : Python 3.6+ Quick Start Installation using pip : pip install foxcross Create some test data and a simple model in the same directory to be served: directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing , run_model_serving class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] if __name__ == \"__main__\" : run_model_serving () Run the model locally python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the list incremented by 1. You can visit localhost:8000/ to see all the available endpoints for your model. Why does this package exist? Currently, some of the most popular data science model building frameworks such as PyTorch and Scikit-Learn do not come with a built in serving library similar to TensorFlow Serving. To fill this gap, people create Flask applications to serve their model. This can be error prone, and the implementation can differ between each model. Additionally, Flask is a WSGI web framework, whereas Foxcross is built on Starlette , a more performant ASGI web framework. Foxcross aims to be the serving library for data science models built with frameworks that do not come with their own serving library. Using Foxcross allows for consistent and testable serving of data science models.","title":"Home"},{"location":"#foxcross","text":"AsyncIO serving for data science models built on Starlette Requirements : Python 3.6+","title":"Foxcross"},{"location":"#quick-start","text":"Installation using pip : pip install foxcross Create some test data and a simple model in the same directory to be served: directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing , run_model_serving class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] if __name__ == \"__main__\" : run_model_serving () Run the model locally python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the list incremented by 1. You can visit localhost:8000/ to see all the available endpoints for your model.","title":"Quick Start"},{"location":"#why-does-this-package-exist","text":"Currently, some of the most popular data science model building frameworks such as PyTorch and Scikit-Learn do not come with a built in serving library similar to TensorFlow Serving. To fill this gap, people create Flask applications to serve their model. This can be error prone, and the implementation can differ between each model. Additionally, Flask is a WSGI web framework, whereas Foxcross is built on Starlette , a more performant ASGI web framework. Foxcross aims to be the serving library for data science models built with frameworks that do not come with their own serving library. Using Foxcross allows for consistent and testable serving of data science models.","title":"Why does this package exist?"},{"location":"pandas-serving/","text":"Pandas Serving Make sure you have installed foxcross with the pandas extra using: pip install foxcross[pandas] Basic Example directory structure . +-- data.json +-- models.py data.json { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] } models.py from foxcross.pandas_serving import DataFrameModelServing , run_pandas_serving import pandas class InterpolateModelServing ( DataFrameModelServing ): test_data_path = \"data.json\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" ) if __name__ == \"__main__\" : run_pandas_serving () Run the model locally: python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the the null values replaced. You can visit localhost:8000/ to see all the available endpoints for your model.","title":"Pandas Serving"},{"location":"pandas-serving/#pandas-serving","text":"Make sure you have installed foxcross with the pandas extra using: pip install foxcross[pandas]","title":"Pandas Serving"},{"location":"pandas-serving/#basic-example","text":"directory structure . +-- data.json +-- models.py data.json { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] } models.py from foxcross.pandas_serving import DataFrameModelServing , run_pandas_serving import pandas class InterpolateModelServing ( DataFrameModelServing ): test_data_path = \"data.json\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" ) if __name__ == \"__main__\" : run_pandas_serving () Run the model locally: python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the the null values replaced. You can visit localhost:8000/ to see all the available endpoints for your model.","title":"Basic Example"},{"location":"release-notes/","text":"0.6.0 (Next release) Added PreProcessingError for use with pre_process_input Added PostProcessingError for use with post_process_results 0.5.0 Removed kubernetes liveness and readiness endpoints Decoupled formatting of input and output data from processing hooks Added new exception for undefined tests data path Fixed ujson OverflowError due to numpy.NaN Renamed NoServingModelsFoundError to NoModelServingFoundError Removed pre processing from /input-format/ endpoint Refactored model serving compose interface Enabled passing of kwargs to ModelServingRunner methods 0.4.0 Fixed pandas import error Reworked running model serving and composing serving models 0.3.0 Reworked API Fixed quick start example 0.2.0 First working release 0.1.0 Initial release","title":"Release Notes"},{"location":"release-notes/#060-next-release","text":"Added PreProcessingError for use with pre_process_input Added PostProcessingError for use with post_process_results","title":"0.6.0 (Next release)"},{"location":"release-notes/#050","text":"Removed kubernetes liveness and readiness endpoints Decoupled formatting of input and output data from processing hooks Added new exception for undefined tests data path Fixed ujson OverflowError due to numpy.NaN Renamed NoServingModelsFoundError to NoModelServingFoundError Removed pre processing from /input-format/ endpoint Refactored model serving compose interface Enabled passing of kwargs to ModelServingRunner methods","title":"0.5.0"},{"location":"release-notes/#040","text":"Fixed pandas import error Reworked running model serving and composing serving models","title":"0.4.0"},{"location":"release-notes/#030","text":"Reworked API Fixed quick start example","title":"0.3.0"},{"location":"release-notes/#020","text":"First working release","title":"0.2.0"},{"location":"release-notes/#010","text":"Initial release","title":"0.1.0"},{"location":"serving/","text":"Serving Basic Overview To serve a data science model with Foxcross , you must do three things: create a class that inherits from ModelServing define a predict method on the class that returns JSON serializable data supply test data by defining the class attribute test_data_path directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] Serving Endpoints Subclassing any class that inherits from ModelServing gives you four endpoints: / (root endpoint) Shows you the different endpoints and HTTP methods for your model Allows you to navigate to those endpoints /predict/ Allows users to POST their input data and receive a prediction from your model /predict-test/ Uses the test_data_path to read your test data and do a prediction with the test data Allows you and your users to test that your prediction is working as expected through a GET request /input-format/ Reads the test_data_path and returns the data through a GET request Allows you and your users to see what the model expects as input for the predict endpoint Serving Hooks Hook Overview Foxcross contains two sets of hooks. One set that happens on serving startup and one set that happens during the models prediction. All subclasses of ModelServing have access to these methods, and all these methods are optional to define. load_model pre_process_input post_process_results Hook Process On startup : run model serving -> load_model -> model serving started This process happens when you start serving your model On prediction : pre_process_input -> predict -> post_process_results This process happens every time the predict and predict-test endpoints are called load_model This method allows you to load your model on startup and into memory . pre_process_input The pre_process_input method allows you to transform your input data prior to a prediction. post_process_results The post_process_results method allows you to transform your prediction results prior to them being returned. Example directory structure . +-- data.json +-- models.py +-- random_forest.pkl models.py from sklearn.externals import joblib from foxcross.serving import ModelServing class RandomForest ( ModelServing ): test_data_path = \"data.json\" def load_model ( self ): self . model = joblib . load ( \"random_forest.pkl\" ) def pre_process_input ( self , data ): return self . add_missing_values ( data ) def add_missing_values ( self , data ): ... def predict ( self , data ): return self . model . predict ( data ) def post_process_results ( self , data ): return self . prep_results ( data ) def prep_results ( self , data ): ...","title":"Serving"},{"location":"serving/#serving","text":"","title":"Serving"},{"location":"serving/#basic-overview","text":"To serve a data science model with Foxcross , you must do three things: create a class that inherits from ModelServing define a predict method on the class that returns JSON serializable data supply test data by defining the class attribute test_data_path directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ]","title":"Basic Overview"},{"location":"serving/#serving-endpoints","text":"Subclassing any class that inherits from ModelServing gives you four endpoints: / (root endpoint) Shows you the different endpoints and HTTP methods for your model Allows you to navigate to those endpoints /predict/ Allows users to POST their input data and receive a prediction from your model /predict-test/ Uses the test_data_path to read your test data and do a prediction with the test data Allows you and your users to test that your prediction is working as expected through a GET request /input-format/ Reads the test_data_path and returns the data through a GET request Allows you and your users to see what the model expects as input for the predict endpoint","title":"Serving Endpoints"},{"location":"serving/#serving-hooks","text":"","title":"Serving Hooks"},{"location":"serving/#hook-overview","text":"Foxcross contains two sets of hooks. One set that happens on serving startup and one set that happens during the models prediction. All subclasses of ModelServing have access to these methods, and all these methods are optional to define. load_model pre_process_input post_process_results","title":"Hook Overview"},{"location":"serving/#hook-process","text":"On startup : run model serving -> load_model -> model serving started This process happens when you start serving your model On prediction : pre_process_input -> predict -> post_process_results This process happens every time the predict and predict-test endpoints are called","title":"Hook Process"},{"location":"serving/#load_model","text":"This method allows you to load your model on startup and into memory .","title":"load_model"},{"location":"serving/#pre_process_input","text":"The pre_process_input method allows you to transform your input data prior to a prediction.","title":"pre_process_input"},{"location":"serving/#post_process_results","text":"The post_process_results method allows you to transform your prediction results prior to them being returned.","title":"post_process_results"},{"location":"serving/#example","text":"directory structure . +-- data.json +-- models.py +-- random_forest.pkl models.py from sklearn.externals import joblib from foxcross.serving import ModelServing class RandomForest ( ModelServing ): test_data_path = \"data.json\" def load_model ( self ): self . model = joblib . load ( \"random_forest.pkl\" ) def pre_process_input ( self , data ): return self . add_missing_values ( data ) def add_missing_values ( self , data ): ... def predict ( self , data ): return self . model . predict ( data ) def post_process_results ( self , data ): return self . prep_results ( data ) def prep_results ( self , data ): ...","title":"Example"}]}