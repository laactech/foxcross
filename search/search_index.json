{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Foxcross AsyncIO serving for data science models built on Starlette Requirements : Python 3.6+ Quick Start Installation using pip : pip install foxcross Create some test data and a simple model in the same directory to be served: directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing , run_model_serving class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] if __name__ == \"__main__\" : run_model_serving () Run the model locally python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the list incremented by 1. You can visit localhost:8000/ to see all the available endpoints for your model. Why does this package exist? Currently, some of the most popular data science model building frameworks such as PyTorch and Scikit-Learn do not come with a built in serving library similar to TensorFlow Serving. To fill this gap, people create Flask applications to serve their model. This can be error prone, and the implementation can differ between each model. Additionally, Flask is a WSGI web framework, whereas Foxcross is built on Starlette , a more performant ASGI web framework. Foxcross aims to be the serving library for data science models built with frameworks that do not come with their own serving library. Using Foxcross enables consistent and testable serving of data science models. Security If you believe you've found a bug with security implications, please do not disclose this issue in a public forum. Email us at support@laac.dev","title":"Home"},{"location":"#foxcross","text":"AsyncIO serving for data science models built on Starlette Requirements : Python 3.6+","title":"Foxcross"},{"location":"#quick-start","text":"Installation using pip : pip install foxcross Create some test data and a simple model in the same directory to be served: directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing , run_model_serving class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] if __name__ == \"__main__\" : run_model_serving () Run the model locally python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the list incremented by 1. You can visit localhost:8000/ to see all the available endpoints for your model.","title":"Quick Start"},{"location":"#why-does-this-package-exist","text":"Currently, some of the most popular data science model building frameworks such as PyTorch and Scikit-Learn do not come with a built in serving library similar to TensorFlow Serving. To fill this gap, people create Flask applications to serve their model. This can be error prone, and the implementation can differ between each model. Additionally, Flask is a WSGI web framework, whereas Foxcross is built on Starlette , a more performant ASGI web framework. Foxcross aims to be the serving library for data science models built with frameworks that do not come with their own serving library. Using Foxcross enables consistent and testable serving of data science models.","title":"Why does this package exist?"},{"location":"#security","text":"If you believe you've found a bug with security implications, please do not disclose this issue in a public forum. Email us at support@laac.dev","title":"Security"},{"location":"advanced-usage/","text":"Advanced Usage Debugging Mode WARNING: this should not be enabled in production from foxcross.serving import run_model_serving run_model_serving ( debug = True ) or from foxcross.serving import compose_models app = compose_models ( debug = True ) Changing the serving module name If you don't want to serve your models from models.py , pass a different module name when running the serving. directory structure . +-- data.json +-- my_awesome_module.py from foxcross.serving import run_model_serving run_model_serving ( module_name = \"my_awesome_module\" ) or from foxcross.serving import compose_models app = compose_models ( module_name = \"my_awesome_module\" ) Requiring HTTPS connections To more securely serve your models, you can require all connections to your model be encrypted via HTTPS from foxcross.serving import run_model_serving run_model_serving ( redirect_https = True ) or from foxcross.serving import compose_models app = compose_models ( redirect_https = True ) Improving performance To help improve performance, Foxcross supports using extra packages. UJSON UJSON is supported to speed up JSON serialization and deserialization. To install ujson with Foxcross, use: pip install foxcross [ ujson ] Modin Modin is supported to speed up pandas operations. To install modin with Foxcross, use: pip install foxcross [ modin ] Overriding the HTTP status code in custom exceptions The custom exceptions, PredictionError , PreProcessingError , and PostProcessingError come with default HTTP status codes returned to the user. These default status codes can be overridden using the http_status_code class attribute from foxcross.serving import ModelServing from foxcross.exceptions import PredictionError class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): try : return [ x + 1 for x in data ] except ValueError as exc : new_exc = PredictionError ( f \"Failed to do prediction: {exc}\" ) new_exc . http_status_code = 500 raise new_exc","title":"Advanced Usage"},{"location":"advanced-usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"advanced-usage/#debugging-mode","text":"WARNING: this should not be enabled in production from foxcross.serving import run_model_serving run_model_serving ( debug = True ) or from foxcross.serving import compose_models app = compose_models ( debug = True )","title":"Debugging Mode"},{"location":"advanced-usage/#changing-the-serving-module-name","text":"If you don't want to serve your models from models.py , pass a different module name when running the serving. directory structure . +-- data.json +-- my_awesome_module.py from foxcross.serving import run_model_serving run_model_serving ( module_name = \"my_awesome_module\" ) or from foxcross.serving import compose_models app = compose_models ( module_name = \"my_awesome_module\" )","title":"Changing the serving module name"},{"location":"advanced-usage/#requiring-https-connections","text":"To more securely serve your models, you can require all connections to your model be encrypted via HTTPS from foxcross.serving import run_model_serving run_model_serving ( redirect_https = True ) or from foxcross.serving import compose_models app = compose_models ( redirect_https = True )","title":"Requiring HTTPS connections"},{"location":"advanced-usage/#improving-performance","text":"To help improve performance, Foxcross supports using extra packages.","title":"Improving performance"},{"location":"advanced-usage/#ujson","text":"UJSON is supported to speed up JSON serialization and deserialization. To install ujson with Foxcross, use: pip install foxcross [ ujson ]","title":"UJSON"},{"location":"advanced-usage/#modin","text":"Modin is supported to speed up pandas operations. To install modin with Foxcross, use: pip install foxcross [ modin ]","title":"Modin"},{"location":"advanced-usage/#overriding-the-http-status-code-in-custom-exceptions","text":"The custom exceptions, PredictionError , PreProcessingError , and PostProcessingError come with default HTTP status codes returned to the user. These default status codes can be overridden using the http_status_code class attribute from foxcross.serving import ModelServing from foxcross.exceptions import PredictionError class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): try : return [ x + 1 for x in data ] except ValueError as exc : new_exc = PredictionError ( f \"Failed to do prediction: {exc}\" ) new_exc . http_status_code = 500 raise new_exc","title":"Overriding the HTTP status code in custom exceptions"},{"location":"contributing/","text":"Contributing to Foxcross To submit new code to the project you'll need to: Fork the repo Clone your fork on your local computer: git clone https://github.com/<username>/foxcross.git Install poetry for managing dependencies Install Foxcross locally: poetry install Run the basic test suite: pytest tests/test_serving.py tests/test_failed_serving.py tests/test_no_extra.py Install the linters: pre-commit install Create a branch off of develop for your work: git checkout develop git checkout -b <branch-name> Make your changes Add any tests or documentation necessary Push to your remote: git push origin <branch-name> Open a pull request","title":"Contributing"},{"location":"contributing/#contributing-to-foxcross","text":"To submit new code to the project you'll need to: Fork the repo Clone your fork on your local computer: git clone https://github.com/<username>/foxcross.git Install poetry for managing dependencies Install Foxcross locally: poetry install Run the basic test suite: pytest tests/test_serving.py tests/test_failed_serving.py tests/test_no_extra.py Install the linters: pre-commit install Create a branch off of develop for your work: git checkout develop git checkout -b <branch-name> Make your changes Add any tests or documentation necessary Push to your remote: git push origin <branch-name> Open a pull request","title":"Contributing to Foxcross"},{"location":"full-examples/","text":"Full Examples Heroku Deployment An example set of Foxcross models deployed on heroku. Deployed application Source code Docker Example An example set of Foxcross models inside a Docker container Source code","title":"Full Examples"},{"location":"full-examples/#full-examples","text":"","title":"Full Examples"},{"location":"full-examples/#heroku-deployment","text":"An example set of Foxcross models deployed on heroku. Deployed application Source code","title":"Heroku Deployment"},{"location":"full-examples/#docker-example","text":"An example set of Foxcross models inside a Docker container Source code","title":"Docker Example"},{"location":"pandas-serving/","text":"Pandas Serving Make sure you have installed Foxcross with the pandas extra using: pip install foxcross[pandas] Overview Serving pandas based models works very similar to the basic model serving with a few caveats: The class to subclass changes from ModelServing to DataFrameModelServing The internal data structure for the model should be either a pandas DataFrame or a dictionary of pandas DataFrames with string keys Running the model serving requires using run_pandas_serving from foxcross.pandas_serving Composing serving models requires using compose_pandas from foxcross.pandas_serving Basic Example directory structure . +-- data.json +-- models.py data.json { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] } models.py from foxcross.pandas_serving import DataFrameModelServing , run_pandas_serving import pandas class InterpolateModel ( DataFrameModelServing ): test_data_path = \"data.json\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" ) if __name__ == \"__main__\" : run_pandas_serving () Run the model locally: python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the the null values replaced. You can visit localhost:8000/ to see all the available endpoints for your model. Serving a dictionary of DataFrames model Foxcross DataFrameModelServing uses either a pandas DataFrame or a dictionary of pandas DataFrames as its data structure. To serve a dictionary of DataFrames, you must add \"multi_dataframe\": true to your input data. Example data.json { \"multi_dataframe\" : true , \"interp_dict\" : { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] }, \"interp_list\" : [ 0.0 , null , -1.0 , 1.0 , null , 2.0 , null , null , 2.0 , 3.0 , null , 9.0 , null , 4.0 , -4.0 , 16.0 ] } models.py from typing import Dict from foxcross.pandas_serving import DataFrameModelServing import pandas class InterpolateMultiDataFrameModel ( DataFrameModelServing ): test_data_path = \"data.json\" def predict ( self , data : Dict [ str , pandas . DataFrame ] ) -> Dict [ str , pandas . DataFrame ]: return { key : value . interpolate ( limit_direction = \"both\" ) for key , value in data . items () } Serving pandas and regular models Foxcross can serve both your regular models that inherit from ModelServing and DataFrameModelServing together. You must use either run_pandas_serving or compose_pandas whenever your models use DataFrameModelServing . Example directory structure . +-- add.json +-- interpolate.json +-- models.py interpolate.json { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] } add.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.pandas_serving import DataFrameModelServing , run_pandas_serving from foxcross.serving import ModelServing import pandas class InterpolateModel ( DataFrameModelServing ): test_data_path = \"interpolate.json\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" ) class AddOneModel ( ModelServing ): test_data_path = \"add.json\" def predict ( self , data ): return [ x + 1 for x in data ] if __name__ == \"__main__\" : run_pandas_serving () Run the model locally: python models.py Navigate to localhost:8000/ in your web browser, and you should see both the /addonemodel and the /interpolatemodel . Changing the orient of Pandas output Foxcross uses the Pandas to_dict method before turning results into JSON. The to_dict method uses an orient argument to determine the output format. The default orient used by Foxcross is index , but this can be changed. Example models.py from foxcross.pandas_serving import DataFrameModelServing import pandas class InterpolateModel ( DataFrameModelServing ): test_data_path = \"data.json\" pandas_orient = \"records\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" )","title":"Pandas Serving"},{"location":"pandas-serving/#pandas-serving","text":"Make sure you have installed Foxcross with the pandas extra using: pip install foxcross[pandas]","title":"Pandas Serving"},{"location":"pandas-serving/#overview","text":"Serving pandas based models works very similar to the basic model serving with a few caveats: The class to subclass changes from ModelServing to DataFrameModelServing The internal data structure for the model should be either a pandas DataFrame or a dictionary of pandas DataFrames with string keys Running the model serving requires using run_pandas_serving from foxcross.pandas_serving Composing serving models requires using compose_pandas from foxcross.pandas_serving","title":"Overview"},{"location":"pandas-serving/#basic-example","text":"directory structure . +-- data.json +-- models.py data.json { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] } models.py from foxcross.pandas_serving import DataFrameModelServing , run_pandas_serving import pandas class InterpolateModel ( DataFrameModelServing ): test_data_path = \"data.json\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" ) if __name__ == \"__main__\" : run_pandas_serving () Run the model locally: python models.py Navigate to localhost:8000/predict-test/ in your web browser, and you should see the the null values replaced. You can visit localhost:8000/ to see all the available endpoints for your model.","title":"Basic Example"},{"location":"pandas-serving/#serving-a-dictionary-of-dataframes-model","text":"Foxcross DataFrameModelServing uses either a pandas DataFrame or a dictionary of pandas DataFrames as its data structure. To serve a dictionary of DataFrames, you must add \"multi_dataframe\": true to your input data.","title":"Serving a dictionary of DataFrames model"},{"location":"pandas-serving/#example","text":"data.json { \"multi_dataframe\" : true , \"interp_dict\" : { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] }, \"interp_list\" : [ 0.0 , null , -1.0 , 1.0 , null , 2.0 , null , null , 2.0 , 3.0 , null , 9.0 , null , 4.0 , -4.0 , 16.0 ] } models.py from typing import Dict from foxcross.pandas_serving import DataFrameModelServing import pandas class InterpolateMultiDataFrameModel ( DataFrameModelServing ): test_data_path = \"data.json\" def predict ( self , data : Dict [ str , pandas . DataFrame ] ) -> Dict [ str , pandas . DataFrame ]: return { key : value . interpolate ( limit_direction = \"both\" ) for key , value in data . items () }","title":"Example"},{"location":"pandas-serving/#serving-pandas-and-regular-models","text":"Foxcross can serve both your regular models that inherit from ModelServing and DataFrameModelServing together. You must use either run_pandas_serving or compose_pandas whenever your models use DataFrameModelServing .","title":"Serving pandas and regular models"},{"location":"pandas-serving/#example_1","text":"directory structure . +-- add.json +-- interpolate.json +-- models.py interpolate.json { \"A\" : [ 12 , 4 , 5 , null , 1 ], \"B\" : [ null , 2 , 54 , 3 , null ], \"C\" : [ 20 , 16 , null , 3 , 8 ], \"D\" : [ 14 , 3 , null , null , 6 ] } add.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.pandas_serving import DataFrameModelServing , run_pandas_serving from foxcross.serving import ModelServing import pandas class InterpolateModel ( DataFrameModelServing ): test_data_path = \"interpolate.json\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" ) class AddOneModel ( ModelServing ): test_data_path = \"add.json\" def predict ( self , data ): return [ x + 1 for x in data ] if __name__ == \"__main__\" : run_pandas_serving () Run the model locally: python models.py Navigate to localhost:8000/ in your web browser, and you should see both the /addonemodel and the /interpolatemodel .","title":"Example"},{"location":"pandas-serving/#changing-the-orient-of-pandas-output","text":"Foxcross uses the Pandas to_dict method before turning results into JSON. The to_dict method uses an orient argument to determine the output format. The default orient used by Foxcross is index , but this can be changed.","title":"Changing the orient of Pandas output"},{"location":"pandas-serving/#example_2","text":"models.py from foxcross.pandas_serving import DataFrameModelServing import pandas class InterpolateModel ( DataFrameModelServing ): test_data_path = \"data.json\" pandas_orient = \"records\" def predict ( self , data : pandas . DataFrame ) -> pandas . DataFrame : return data . interpolate ( limit_direction = \"both\" )","title":"Example"},{"location":"release-notes/","text":"0.8.0 Added download form for input-format and predict-test endpoints Changed HTTP GET on input-format and predict-test to only return HTML Added HTTP POST on input-format and predict-test which will return JSON Removed starlette_kwargs and uvicorn_kwargs from run_model_serving and run_pandas_serving Fixed bug with navbar paths for single model serving 0.7.1 Fixed serving the wrong template for predict-test endpoint 0.7.0 Improved exception handling Added more logging Renamed BadDataFormatError to PredictionError Updated route name generation in runner.ModelServingRunner.compose to create better urls Added bootstrap 4 to improve model frontend Created HTML output for all endpoints Split kwargs into starlette_kwargs and uvicorn_kwargs in run_model_serving and run_pandas_serving 0.6.0 Added PreProcessingError for use with pre_process_input Added PostProcessingError for use with post_process_results Included home route for every model index page Removed performance extra install Refactored model serving compose interface Added ability to override HTTP status code in custom exceptions 0.5.0 Removed kubernetes liveness and readiness endpoints Decoupled formatting of input and output data from processing hooks Added new exception for undefined tests data path Fixed ujson OverflowError due to numpy.NaN Renamed NoServingModelsFoundError to NoModelServingFoundError Removed pre processing from /input-format/ endpoint Refactored model serving compose interface Enabled passing of kwargs to ModelServingRunner methods 0.4.0 Fixed pandas import error Reworked running model serving and composing serving models 0.3.0 Reworked API Fixed quick start example 0.2.0 First working release 0.1.0 Initial release","title":"Release Notes"},{"location":"release-notes/#080","text":"Added download form for input-format and predict-test endpoints Changed HTTP GET on input-format and predict-test to only return HTML Added HTTP POST on input-format and predict-test which will return JSON Removed starlette_kwargs and uvicorn_kwargs from run_model_serving and run_pandas_serving Fixed bug with navbar paths for single model serving","title":"0.8.0"},{"location":"release-notes/#071","text":"Fixed serving the wrong template for predict-test endpoint","title":"0.7.1"},{"location":"release-notes/#070","text":"Improved exception handling Added more logging Renamed BadDataFormatError to PredictionError Updated route name generation in runner.ModelServingRunner.compose to create better urls Added bootstrap 4 to improve model frontend Created HTML output for all endpoints Split kwargs into starlette_kwargs and uvicorn_kwargs in run_model_serving and run_pandas_serving","title":"0.7.0"},{"location":"release-notes/#060","text":"Added PreProcessingError for use with pre_process_input Added PostProcessingError for use with post_process_results Included home route for every model index page Removed performance extra install Refactored model serving compose interface Added ability to override HTTP status code in custom exceptions","title":"0.6.0"},{"location":"release-notes/#050","text":"Removed kubernetes liveness and readiness endpoints Decoupled formatting of input and output data from processing hooks Added new exception for undefined tests data path Fixed ujson OverflowError due to numpy.NaN Renamed NoServingModelsFoundError to NoModelServingFoundError Removed pre processing from /input-format/ endpoint Refactored model serving compose interface Enabled passing of kwargs to ModelServingRunner methods","title":"0.5.0"},{"location":"release-notes/#040","text":"Fixed pandas import error Reworked running model serving and composing serving models","title":"0.4.0"},{"location":"release-notes/#030","text":"Reworked API Fixed quick start example","title":"0.3.0"},{"location":"release-notes/#020","text":"First working release","title":"0.2.0"},{"location":"release-notes/#010","text":"Initial release","title":"0.1.0"},{"location":"serving/","text":"Serving Basic Overview To serve a data science model with Foxcross, you must do three things: create a class that inherits from ModelServing define a predict method on the class that returns JSON serializable data supply JSON test data by defining the class attribute test_data_path directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] Serving Endpoints Subclassing ModelServing gives you four endpoints: / (root endpoint) Shows you the different endpoints and HTTP methods for your model Allows you to navigate to those endpoints /predict/ Allows users to POST their input data and receive a prediction from your model /predict-test/ Uses the test_data_path to read your test data and do a prediction with the test data Allows you and your users to test that your prediction is working as expected through a GET request /input-format/ Reads the test_data_path and returns the data through a GET request Allows you and your users to see what the model expects as input for the predict endpoint Serving Hooks Hook Overview Foxcross contains two sets of hooks. One set that happens on serving startup and one set that happens during the models prediction. All subclasses of ModelServing have access to these methods, and all these methods are optional to define. load_model Allows you to load your model on startup and into memory . pre_process_input Allows you to transform your input data prior to a prediction. post_process_results Allows you to transform your prediction results prior to them being returned. Hook Process On startup : run model serving -> load_model -> model serving started This process happens when you start serving your model On prediction : pre_process_input -> predict -> post_process_results This process happens every time the predict and predict-test endpoints are called Example directory structure . +-- data.json +-- models.py +-- random_forest.pkl models.py from sklearn.externals import joblib from foxcross.serving import ModelServing class RandomForest ( ModelServing ): test_data_path = \"data.json\" def load_model ( self ): self . model = joblib . load ( \"random_forest.pkl\" ) def pre_process_input ( self , data ): return self . add_missing_values ( data ) def add_missing_values ( self , data ): ... def predict ( self , data ): return self . model . predict ( data ) def post_process_results ( self , data ): return self . prep_results ( data ) def prep_results ( self , data ): ... Exception Handling Foxcross comes with custom exceptions for the various methods on the ModelServing class. When raised, the exception message and the correct HTTP status code are returned to the user. Custom Exceptions PredictionError Raise inside of the predict method when an issue occurs when performing the prediction PreProcessingError Raise inside the pre_process_input method when an issue with pre processing the input occurs PostProcessingError Raise inside the post_process_results method when an issue with post processing the results occurs Example from sklearn.externals import joblib from foxcross.serving import ModelServing from foxcross.exceptions import ( PredictionError , PostProcessingError , PreProcessingError , ) class RandomForest ( ModelServing ): test_data_path = \"data.json\" def load_model ( self ): self . model = joblib . load ( \"random_forest.pkl\" ) def pre_process_input ( self , data ): try : return self . add_missing_values ( data ) except ValueError as exc : raise PreProcessingError ( f \"Issue with pre processing the data: {exc}\" ) def add_missing_values ( self , data ): ... def predict ( self , data ): try : return self . model . predict ( data ) except KeyError as exc : raise PredictionError ( f \"Incorrect data format. Missing key {exc}\" ) def post_process_results ( self , data ): try : return self . prep_results ( data ) except TypeError as exc : raise PostProcessingError ( f \"Wrong type: {exc}\" ) def prep_results ( self , data ): ... Serving multiple models Foxcross enables you to compose and serve multiple models from a single place. Example directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing , run_model_serving class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] class AddTwoModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ y + 2 for y in data ] if __name__ == \"__main__\" : run_model_serving () Navigate to localhost:8000/ in your web browser. You should see routes to both the AddOneModel and the AddTwoModel . Clicking on one of the model routes show you that both models come with the same set of endpoints and both perform predictions. How does this work? Foxcross finds all classes inside your models.py file that subclass ModelServing and combines those into a single model serving. Foxcross uses the name of the class such as AddOneModel and AddTwoModel to define the routes where those models live. Authentication Foxcross comes with no built in authentication, and we recommend running Foxcross models inside a trusted environment. Foxcross serving models are simply Starlette applications, and Starlette comes with an interface to achieve authentication. Running in Production Foxcross leverages uvicorn to run model serving. We recommend using gunicorn to serve models in production. Details about running uvicorn with gunicorn can be found here Example directory structure . +-- data.json +-- models.py +-- app.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] class AddTwoModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ y + 2 for y in data ] app.py from foxcross.serving import compose_models app = compose_models () Assuming gunicorn has been installed, run: gunicorn -k uvicorn.workers.UvicornWorker app:app","title":"Serving"},{"location":"serving/#serving","text":"","title":"Serving"},{"location":"serving/#basic-overview","text":"To serve a data science model with Foxcross, you must do three things: create a class that inherits from ModelServing define a predict method on the class that returns JSON serializable data supply JSON test data by defining the class attribute test_data_path directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ]","title":"Basic Overview"},{"location":"serving/#serving-endpoints","text":"Subclassing ModelServing gives you four endpoints: / (root endpoint) Shows you the different endpoints and HTTP methods for your model Allows you to navigate to those endpoints /predict/ Allows users to POST their input data and receive a prediction from your model /predict-test/ Uses the test_data_path to read your test data and do a prediction with the test data Allows you and your users to test that your prediction is working as expected through a GET request /input-format/ Reads the test_data_path and returns the data through a GET request Allows you and your users to see what the model expects as input for the predict endpoint","title":"Serving Endpoints"},{"location":"serving/#serving-hooks","text":"","title":"Serving Hooks"},{"location":"serving/#hook-overview","text":"Foxcross contains two sets of hooks. One set that happens on serving startup and one set that happens during the models prediction. All subclasses of ModelServing have access to these methods, and all these methods are optional to define. load_model Allows you to load your model on startup and into memory . pre_process_input Allows you to transform your input data prior to a prediction. post_process_results Allows you to transform your prediction results prior to them being returned.","title":"Hook Overview"},{"location":"serving/#hook-process","text":"On startup : run model serving -> load_model -> model serving started This process happens when you start serving your model On prediction : pre_process_input -> predict -> post_process_results This process happens every time the predict and predict-test endpoints are called","title":"Hook Process"},{"location":"serving/#example","text":"directory structure . +-- data.json +-- models.py +-- random_forest.pkl models.py from sklearn.externals import joblib from foxcross.serving import ModelServing class RandomForest ( ModelServing ): test_data_path = \"data.json\" def load_model ( self ): self . model = joblib . load ( \"random_forest.pkl\" ) def pre_process_input ( self , data ): return self . add_missing_values ( data ) def add_missing_values ( self , data ): ... def predict ( self , data ): return self . model . predict ( data ) def post_process_results ( self , data ): return self . prep_results ( data ) def prep_results ( self , data ): ...","title":"Example"},{"location":"serving/#exception-handling","text":"Foxcross comes with custom exceptions for the various methods on the ModelServing class. When raised, the exception message and the correct HTTP status code are returned to the user.","title":"Exception Handling"},{"location":"serving/#custom-exceptions","text":"PredictionError Raise inside of the predict method when an issue occurs when performing the prediction PreProcessingError Raise inside the pre_process_input method when an issue with pre processing the input occurs PostProcessingError Raise inside the post_process_results method when an issue with post processing the results occurs","title":"Custom Exceptions"},{"location":"serving/#example_1","text":"from sklearn.externals import joblib from foxcross.serving import ModelServing from foxcross.exceptions import ( PredictionError , PostProcessingError , PreProcessingError , ) class RandomForest ( ModelServing ): test_data_path = \"data.json\" def load_model ( self ): self . model = joblib . load ( \"random_forest.pkl\" ) def pre_process_input ( self , data ): try : return self . add_missing_values ( data ) except ValueError as exc : raise PreProcessingError ( f \"Issue with pre processing the data: {exc}\" ) def add_missing_values ( self , data ): ... def predict ( self , data ): try : return self . model . predict ( data ) except KeyError as exc : raise PredictionError ( f \"Incorrect data format. Missing key {exc}\" ) def post_process_results ( self , data ): try : return self . prep_results ( data ) except TypeError as exc : raise PostProcessingError ( f \"Wrong type: {exc}\" ) def prep_results ( self , data ): ...","title":"Example"},{"location":"serving/#serving-multiple-models","text":"Foxcross enables you to compose and serve multiple models from a single place.","title":"Serving multiple models"},{"location":"serving/#example_2","text":"directory structure . +-- data.json +-- models.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing , run_model_serving class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] class AddTwoModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ y + 2 for y in data ] if __name__ == \"__main__\" : run_model_serving () Navigate to localhost:8000/ in your web browser. You should see routes to both the AddOneModel and the AddTwoModel . Clicking on one of the model routes show you that both models come with the same set of endpoints and both perform predictions.","title":"Example"},{"location":"serving/#how-does-this-work","text":"Foxcross finds all classes inside your models.py file that subclass ModelServing and combines those into a single model serving. Foxcross uses the name of the class such as AddOneModel and AddTwoModel to define the routes where those models live.","title":"How does this work?"},{"location":"serving/#authentication","text":"Foxcross comes with no built in authentication, and we recommend running Foxcross models inside a trusted environment. Foxcross serving models are simply Starlette applications, and Starlette comes with an interface to achieve authentication.","title":"Authentication"},{"location":"serving/#running-in-production","text":"Foxcross leverages uvicorn to run model serving. We recommend using gunicorn to serve models in production. Details about running uvicorn with gunicorn can be found here","title":"Running in Production"},{"location":"serving/#example_3","text":"directory structure . +-- data.json +-- models.py +-- app.py data.json [ 1 , 2 , 3 , 4 , 5 ] models.py from foxcross.serving import ModelServing class AddOneModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ x + 1 for x in data ] class AddTwoModel ( ModelServing ): test_data_path = \"data.json\" def predict ( self , data ): return [ y + 2 for y in data ] app.py from foxcross.serving import compose_models app = compose_models () Assuming gunicorn has been installed, run: gunicorn -k uvicorn.workers.UvicornWorker app:app","title":"Example"}]}